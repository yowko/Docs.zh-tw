---
title: 如何選擇 ML.NET 演算法
description: 了解如何選擇機器學習模型的 ML.NET 演算法
author: natke
ms.topic: overview
ms.date: 04/20/1029
ms.openlocfilehash: d1c637437a7b285f2b66b597d616fcf39248697f
ms.sourcegitcommit: 682c64df0322c7bda016f8bfea8954e9b31f1990
ms.translationtype: HT
ms.contentlocale: zh-TW
ms.lasthandoff: 05/13/2019
ms.locfileid: "65557788"
---
# <a name="how-to-choose-an-mlnet-algorithm"></a><span data-ttu-id="71bd1-103">如何選擇 ML.NET 演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-103">How to choose an ML.NET algorithm</span></span>

<span data-ttu-id="71bd1-104">針對每項 [ML.NET 工作](resources/tasks.md)，有多個定型演算法可供選擇。</span><span class="sxs-lookup"><span data-stu-id="71bd1-104">For each [ML.NET task](resources/tasks.md), there are multiple training algorithms to choose from.</span></span> <span data-ttu-id="71bd1-105">要選擇哪一個，取決於您嘗試解決的問題、您資料的特性，以及您目前可使用的計算和儲存資源。</span><span class="sxs-lookup"><span data-stu-id="71bd1-105">Which one to choose depends on the problem you are trying to solve, the characteristics of your data, and the compute and storage resources you have available.</span></span> <span data-ttu-id="71bd1-106">請務必注意，定型的機器學習模型是一種反覆運算程序。</span><span class="sxs-lookup"><span data-stu-id="71bd1-106">It is important to note that training a machine learning model is an iterative process.</span></span> <span data-ttu-id="71bd1-107">您可能需要嘗試多種演算法，找出最適合的那一種。</span><span class="sxs-lookup"><span data-stu-id="71bd1-107">You might need to try multiple algorithms to find the one that works best.</span></span>

<span data-ttu-id="71bd1-108">演算法作用於**特性**。</span><span class="sxs-lookup"><span data-stu-id="71bd1-108">Algorithms operate on **features**.</span></span> <span data-ttu-id="71bd1-109">特性是計算輸入資料所得出的數值。</span><span class="sxs-lookup"><span data-stu-id="71bd1-109">Features are numerical values computed from your input data.</span></span> <span data-ttu-id="71bd1-110">它們是機器學習演算法的最佳輸入。</span><span class="sxs-lookup"><span data-stu-id="71bd1-110">They are optimal inputs for machine learning algorithms.</span></span> <span data-ttu-id="71bd1-111">您使用一或多種[資料轉換](resources/transforms.md)，將未經處理的輸入資料轉換成特性。</span><span class="sxs-lookup"><span data-stu-id="71bd1-111">You transform your raw input data into features using one or more [data transforms](resources/transforms.md).</span></span> <span data-ttu-id="71bd1-112">例如，文字資料會轉換成一組字數統計和字詞組合統計。</span><span class="sxs-lookup"><span data-stu-id="71bd1-112">For example, text data is transformed into a set of word counts and word combination counts.</span></span> <span data-ttu-id="71bd1-113">一旦使用資料轉換從未經處理的資料類型擷取特性，它們就稱為**凸顯**。</span><span class="sxs-lookup"><span data-stu-id="71bd1-113">Once the features have been extracted from a raw data type using data transforms, they are referred to as **featurized**.</span></span> <span data-ttu-id="71bd1-114">例如，凸顯的文字或影像資料。</span><span class="sxs-lookup"><span data-stu-id="71bd1-114">For example, featurized text, or featurized image data.</span></span>

## <a name="trainer--algorithm--task"></a><span data-ttu-id="71bd1-115">定型器 = 演算法 + 工作</span><span class="sxs-lookup"><span data-stu-id="71bd1-115">Trainer = Algorithm + Task</span></span>

<span data-ttu-id="71bd1-116">演算法是為產生**模型**所執行的數學。</span><span class="sxs-lookup"><span data-stu-id="71bd1-116">An algorithm is the math that executes to produce a **model**.</span></span> <span data-ttu-id="71bd1-117">不同演算法產生不同特性的模型。</span><span class="sxs-lookup"><span data-stu-id="71bd1-117">Different algorithms produce models with different characteristics.</span></span> 

<span data-ttu-id="71bd1-118">使用 ML.NET，相同演算法可以套用到不同的工作。</span><span class="sxs-lookup"><span data-stu-id="71bd1-118">With ML.NET, the same algorithm can be applied to different tasks.</span></span> <span data-ttu-id="71bd1-119">例如，隨機對偶座標上升法可用於二元分類、多元分類和迴歸。</span><span class="sxs-lookup"><span data-stu-id="71bd1-119">For example, Stochastic Descent Coordinated Ascent can be used for Binary Classification, Multiclass Classification, and Regression.</span></span> <span data-ttu-id="71bd1-120">不同之處在於如何解譯演算法的輸出，使符合工作。</span><span class="sxs-lookup"><span data-stu-id="71bd1-120">The difference is in how the output of the algorithm is interpreted to match the task.</span></span> 

<span data-ttu-id="71bd1-121">針對每種演算法/工作組合，ML.NET 提供執行定型演算法和完成解譯的元件。</span><span class="sxs-lookup"><span data-stu-id="71bd1-121">For each algorithm/task combination, ML.NET provides a component that executes the training algorithm and does the interpretation.</span></span> <span data-ttu-id="71bd1-122">這些元件稱為「定型器」。</span><span class="sxs-lookup"><span data-stu-id="71bd1-122">These components are called trainers.</span></span> <span data-ttu-id="71bd1-123">例如，<xref:Microsoft.ML.Trainers.SdcaRegressionTrainer> 使用 **StochasticDualCoordinatedAscent** 演算法套用至**迴歸**工作。</span><span class="sxs-lookup"><span data-stu-id="71bd1-123">For example, the <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer> uses the **StochasticDualCoordinatedAscent** algorithm applied to the **Regression** task.</span></span>

## <a name="linear-algorithms"></a><span data-ttu-id="71bd1-124">線性演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-124">Linear algorithms</span></span>

<span data-ttu-id="71bd1-125">線性演算法產生的模型會計算輸入資料和一組**權數**之線性組合的**分數**。</span><span class="sxs-lookup"><span data-stu-id="71bd1-125">Linear algorithms produce a model that calculates **scores** from a linear combination of the input data and a set of **weights**.</span></span> <span data-ttu-id="71bd1-126">權數是在定型期間評估的模型參數。</span><span class="sxs-lookup"><span data-stu-id="71bd1-126">The weights are parameters of the model estimated during training.</span></span>

<span data-ttu-id="71bd1-127">線性演算法適用於[線性可分](https://en.wikipedia.org/wiki/Linear_separability)的特性。</span><span class="sxs-lookup"><span data-stu-id="71bd1-127">Linear algorithms work well for features that are [linearly separable](https://en.wikipedia.org/wiki/Linear_separability).</span></span>

<span data-ttu-id="71bd1-128">使用線性演算法定型之前，應先將特性標準化。</span><span class="sxs-lookup"><span data-stu-id="71bd1-128">Before training with a linear algorithm, the features should be normalized.</span></span> <span data-ttu-id="71bd1-129">這可防止某項特色對結果的影響超過其他特色。</span><span class="sxs-lookup"><span data-stu-id="71bd1-129">This prevents one feature having more influence over the result than others.</span></span>

<span data-ttu-id="71bd1-130">一般而言，線性演算法可調整又快速，定型和預測的成本低廉。</span><span class="sxs-lookup"><span data-stu-id="71bd1-130">In general linear algorithms are scalable and fast, cheap to train, cheap to predict.</span></span> <span data-ttu-id="71bd1-131">它們可以調整特色數目，約為定型資料集的大小。</span><span class="sxs-lookup"><span data-stu-id="71bd1-131">They scale by the number of features and approximately by the size of the training data set.</span></span>

<span data-ttu-id="71bd1-132">線性演算法對定型資料進行多次傳遞。</span><span class="sxs-lookup"><span data-stu-id="71bd1-132">Linear algorithms make multiple passes over the training data.</span></span> <span data-ttu-id="71bd1-133">如果您的資料集貼合記憶體，則先將[快取檢查點](xref:Microsoft.ML.LearningPipelineExtensions.AppendCacheCheckpoint*)新增至 ML.NET 管線，再附加定型器，會讓定型執行更快速。</span><span class="sxs-lookup"><span data-stu-id="71bd1-133">If your dataset fits into memory, then adding a [cache checkpoint](xref:Microsoft.ML.LearningPipelineExtensions.AppendCacheCheckpoint*) to your ML.NET pipeline before appending the trainer, will make the training run faster.</span></span>

<span data-ttu-id="71bd1-134">**線性定型器**</span><span class="sxs-lookup"><span data-stu-id="71bd1-134">**Linear Trainers**</span></span>

|<span data-ttu-id="71bd1-135">演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-135">Algorithm</span></span>|<span data-ttu-id="71bd1-136">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-136">Properties</span></span>|<span data-ttu-id="71bd1-137">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-137">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="71bd1-138">平均感知器</span><span class="sxs-lookup"><span data-stu-id="71bd1-138">Averaged perceptron</span></span>|<span data-ttu-id="71bd1-139">最適合文字分類</span><span class="sxs-lookup"><span data-stu-id="71bd1-139">Best for text classification</span></span>|<xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer>|
|<span data-ttu-id="71bd1-140">隨機對偶座標上升法</span><span class="sxs-lookup"><span data-stu-id="71bd1-140">Stochastic descent coordinated ascent</span></span>|<span data-ttu-id="71bd1-141">好的預設效能不需要微調</span><span class="sxs-lookup"><span data-stu-id="71bd1-141">Tuning not needed for good default performance</span></span>|<span data-ttu-id="71bd1-142"><xref:Microsoft.ML.Trainers.SdcaLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-142"><xref:Microsoft.ML.Trainers.SdcaLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer></span></span>|
|<span data-ttu-id="71bd1-143">L-BFGS</span><span class="sxs-lookup"><span data-stu-id="71bd1-143">L-BFGS</span></span>|<span data-ttu-id="71bd1-144">當特性數目很大時使用。</span><span class="sxs-lookup"><span data-stu-id="71bd1-144">Use when number of features is large.</span></span> <span data-ttu-id="71bd1-145">產生羅吉斯迴歸定型統計資料，但不像 AveragedPerceptronTrainer 縮放自如</span><span class="sxs-lookup"><span data-stu-id="71bd1-145">Produces logistic regression training statistics, but doesn't scale as well as the AveragedPerceptronTrainer</span></span>|<span data-ttu-id="71bd1-146"><xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.LbfgsPoissonRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-146"><xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.LbfgsPoissonRegressionTrainer></span></span>|
|<span data-ttu-id="71bd1-147">符號隨機梯度下降</span><span class="sxs-lookup"><span data-stu-id="71bd1-147">Symbolic stochastic gradient descent</span></span>|<span data-ttu-id="71bd1-148">最快速且最精確的線性二元分類定型器。</span><span class="sxs-lookup"><span data-stu-id="71bd1-148">Fastest and most accurate linear binary classification trainer.</span></span> <span data-ttu-id="71bd1-149">可隨處理器數目調整</span><span class="sxs-lookup"><span data-stu-id="71bd1-149">Scales well with number of processors</span></span>|<xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>|

## <a name="decision-tree-algorithms"></a><span data-ttu-id="71bd1-150">決策樹演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-150">Decision tree algorithms</span></span>

<span data-ttu-id="71bd1-151">決策樹演算法建立的模型，包含一系列的決策：所有資料值的有效流程圖。</span><span class="sxs-lookup"><span data-stu-id="71bd1-151">Decision tree algorithms create a model that contains a series of decisions: effectively a flow chart through the data values.</span></span>

<span data-ttu-id="71bd1-152">特性不需要為線性可分，也可以使用這種演算法。</span><span class="sxs-lookup"><span data-stu-id="71bd1-152">Features do not need to be linearly separable to use this type of algorithm.</span></span> <span data-ttu-id="71bd1-153">而且特性不需要標準化，因為在決策流程中會單獨使用特性向量的個別值。</span><span class="sxs-lookup"><span data-stu-id="71bd1-153">And features do not need to be normalized, because the individual values in the feature vector are used independently in the decision process.</span></span>

<span data-ttu-id="71bd1-154">決策樹演算法通常非常精確。</span><span class="sxs-lookup"><span data-stu-id="71bd1-154">Decision tree algorithms are generally very accurate.</span></span>

<span data-ttu-id="71bd1-155">除了一般化累加模型 (GAM) 外，樹狀模型在特性數目很大時會欠缺解釋性。</span><span class="sxs-lookup"><span data-stu-id="71bd1-155">Except for Generalized Additive Models (GAMs), tree models can lack explainability when the number of features is large.</span></span>

<span data-ttu-id="71bd1-156">決策樹演算法需要更多資源，且不像線性演算法縮放自如。</span><span class="sxs-lookup"><span data-stu-id="71bd1-156">Decision tree algorithms take more resources and do not scale as well as linear ones do.</span></span> <span data-ttu-id="71bd1-157">它們也很適合貼近記憶體的資料集。</span><span class="sxs-lookup"><span data-stu-id="71bd1-157">They do perform well on datasets that can fit into memory.</span></span>

<span data-ttu-id="71bd1-158">促進式決策樹是一整團的小型樹狀結構，其中每個樹狀結構都會評分輸入資料，並將分數傳遞到下一個樹狀結構，以產生更好的分數，整體中的每個樹狀結構都可以改善前一個樹狀結構。</span><span class="sxs-lookup"><span data-stu-id="71bd1-158">Boosted decision trees are an ensemble of small trees where each tree scores the input data and passes the score onto the next tree to produce a better score, and so on, where each tree in the ensemble improves on the previous.</span></span>

<span data-ttu-id="71bd1-159">**決策樹定型器**</span><span class="sxs-lookup"><span data-stu-id="71bd1-159">**Decision tree trainers**</span></span>

|<span data-ttu-id="71bd1-160">演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-160">Algorithm</span></span>|<span data-ttu-id="71bd1-161">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-161">Properties</span></span>|<span data-ttu-id="71bd1-162">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-162">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="71bd1-163">輕量型梯度提升機器</span><span class="sxs-lookup"><span data-stu-id="71bd1-163">Light gradient boosted machine</span></span>|<span data-ttu-id="71bd1-164">最快速且最精確的二元分類樹狀定型器。</span><span class="sxs-lookup"><span data-stu-id="71bd1-164">Fastest and most accurate of the binary classification tree trainers.</span></span> <span data-ttu-id="71bd1-165">微調程度高</span><span class="sxs-lookup"><span data-stu-id="71bd1-165">Highly tunable</span></span>|<span data-ttu-id="71bd1-166"><xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmMulticlassTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRegressionTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRankingTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-166"><xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmMulticlassTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRegressionTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRankingTrainer></span></span>|
|<span data-ttu-id="71bd1-167">快速的樹狀結構</span><span class="sxs-lookup"><span data-stu-id="71bd1-167">Fast tree</span></span>|<span data-ttu-id="71bd1-168">用於特徵化影像資料。</span><span class="sxs-lookup"><span data-stu-id="71bd1-168">Use for featurized image data.</span></span> <span data-ttu-id="71bd1-169">復原不對稱的資料。</span><span class="sxs-lookup"><span data-stu-id="71bd1-169">Resilient to unbalanced data.</span></span> <span data-ttu-id="71bd1-170">微調程度高</span><span class="sxs-lookup"><span data-stu-id="71bd1-170">Highly tunable</span></span> | <span data-ttu-id="71bd1-171"><xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRegressionTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeTweedieTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRankingTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-171"><xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRegressionTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeTweedieTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRankingTrainer></span></span>|
|<span data-ttu-id="71bd1-172">快速樹系</span><span class="sxs-lookup"><span data-stu-id="71bd1-172">Fast forest</span></span>|<span data-ttu-id="71bd1-173">適用於有很多雜訊的資料</span><span class="sxs-lookup"><span data-stu-id="71bd1-173">Works well with noisy data</span></span>|<span data-ttu-id="71bd1-174"><xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastForestRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-174"><xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastForestRegressionTrainer></span></span>|
|<span data-ttu-id="71bd1-175">一般化累加模型 (GAM)</span><span class="sxs-lookup"><span data-stu-id="71bd1-175">Generalized additive model (GAM)</span></span>|<span data-ttu-id="71bd1-176">最適合處理適用樹狀演算法，但解釋性優先的問題</span><span class="sxs-lookup"><span data-stu-id="71bd1-176">Best for problems that perform well with tree algorithms but where explainability is a priority</span></span>|<span data-ttu-id="71bd1-177"><xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.GamRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-177"><xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.GamRegressionTrainer></span></span>|

## <a name="matrix-factorization"></a><span data-ttu-id="71bd1-178">矩陣分解</span><span class="sxs-lookup"><span data-stu-id="71bd1-178">Matrix factorization</span></span>

|<span data-ttu-id="71bd1-179">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-179">Properties</span></span>|<span data-ttu-id="71bd1-180">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-180">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="71bd1-181">最適合大型資料集的分類疏鬆資料</span><span class="sxs-lookup"><span data-stu-id="71bd1-181">Best for sparse categorical data, with large datasets</span></span>|<xref:Microsoft.ML.Trainers.FieldAwareFactorizationMachineTrainer>|

## <a name="meta-algorithms"></a><span data-ttu-id="71bd1-182">中繼演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-182">Meta algorithms</span></span>

<span data-ttu-id="71bd1-183">這些定型器從二元定型器建立多元定型器。</span><span class="sxs-lookup"><span data-stu-id="71bd1-183">These trainers create a multi-class trainer from a binary trainer.</span></span> <span data-ttu-id="71bd1-184">搭配使用 <xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer><xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer>、<xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>、<xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer><xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer><xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer>、<xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer>。</span><span class="sxs-lookup"><span data-stu-id="71bd1-184">Use with <xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer>, <xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer>.</span></span>

|<span data-ttu-id="71bd1-185">演算法</span><span class="sxs-lookup"><span data-stu-id="71bd1-185">Algorithm</span></span>|<span data-ttu-id="71bd1-186">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-186">Properties</span></span>|<span data-ttu-id="71bd1-187">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-187">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="71bd1-188">一對多</span><span class="sxs-lookup"><span data-stu-id="71bd1-188">One versus all</span></span>|<span data-ttu-id="71bd1-189">此多元分類器每類別定型一個二元分類器，從所有其他類別中區分出該類別。</span><span class="sxs-lookup"><span data-stu-id="71bd1-189">This multiclass classifier trains one binary classifier for each class, which distinguishes that class from all other classes.</span></span> <span data-ttu-id="71bd1-190">規模受限於要分類的類別數目</span><span class="sxs-lookup"><span data-stu-id="71bd1-190">Is limited in scale by the number of classes to categorize</span></span>|[<span data-ttu-id="71bd1-191">OneVersusAllTrainer\<BinaryClassificationTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-191">OneVersusAllTrainer\<BinaryClassificationTrainer></span></span>](xref:Microsoft.ML.Trainers.OneVersusAllTrainer) |
|<span data-ttu-id="71bd1-192">成對結合</span><span class="sxs-lookup"><span data-stu-id="71bd1-192">Pairwise coupling</span></span>|<span data-ttu-id="71bd1-193">此多元分類器針對每對類別定型二元分類演算法。</span><span class="sxs-lookup"><span data-stu-id="71bd1-193">This multiclass classifier trains a binary classification algorithm on each pair of classes.</span></span> <span data-ttu-id="71bd1-194">規模受限於類別數目，因為必須定型每對類別的組合。</span><span class="sxs-lookup"><span data-stu-id="71bd1-194">Is limited in scale by the number of classes, as each combination of two classes must be trained.</span></span>|[<span data-ttu-id="71bd1-195">PairwiseCouplingTrainer\<BinaryClassificationTrainer></span><span class="sxs-lookup"><span data-stu-id="71bd1-195">PairwiseCouplingTrainer\<BinaryClassificationTrainer></span></span>](xref:Microsoft.ML.Trainers.PairwiseCouplingTrainer)|

## <a name="k-means"></a><span data-ttu-id="71bd1-196">K-Means</span><span class="sxs-lookup"><span data-stu-id="71bd1-196">K-Means</span></span>

|<span data-ttu-id="71bd1-197">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-197">Properties</span></span>|<span data-ttu-id="71bd1-198">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-198">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="71bd1-199">用於叢集</span><span class="sxs-lookup"><span data-stu-id="71bd1-199">Use for clustering</span></span>|<xref:Microsoft.ML.Trainers.KMeansTrainer>|

## <a name="principal-component-analysis"></a><span data-ttu-id="71bd1-200">主體元件分析</span><span class="sxs-lookup"><span data-stu-id="71bd1-200">Principal component analysis</span></span>

|<span data-ttu-id="71bd1-201">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-201">Properties</span></span>|<span data-ttu-id="71bd1-202">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-202">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="71bd1-203">用於異常偵測</span><span class="sxs-lookup"><span data-stu-id="71bd1-203">Use for anomaly detection</span></span>|<xref:Microsoft.ML.Trainers.RandomizedPcaTrainer>|

## <a name="naive-bayes"></a><span data-ttu-id="71bd1-204">貝氏機率分類</span><span class="sxs-lookup"><span data-stu-id="71bd1-204">Naive Bayes</span></span>

|<span data-ttu-id="71bd1-205">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-205">Properties</span></span>|<span data-ttu-id="71bd1-206">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-206">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="71bd1-207">當特性獨立存在且定型資料集很小時，請使用此多元分類定型器。</span><span class="sxs-lookup"><span data-stu-id="71bd1-207">Use this multi-class classification trainer when the features are independent, and the training dataset is small.</span></span>|<xref:Microsoft.ML.Trainers.NaiveBayesMulticlassTrainer>|

## <a name="prior-trainer"></a><span data-ttu-id="71bd1-208">舊的定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-208">Prior Trainer</span></span>

|<span data-ttu-id="71bd1-209">屬性</span><span class="sxs-lookup"><span data-stu-id="71bd1-209">Properties</span></span>|<span data-ttu-id="71bd1-210">定型器</span><span class="sxs-lookup"><span data-stu-id="71bd1-210">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="71bd1-211">使用此二元分類定型器建立其他定型器的效能基準。</span><span class="sxs-lookup"><span data-stu-id="71bd1-211">Use this binary classification trainer to baseline the performance of other trainers.</span></span> <span data-ttu-id="71bd1-212">為有效率，其他定型器的計量應該比舊定型器好。</span><span class="sxs-lookup"><span data-stu-id="71bd1-212">To be effective, the metrics of the other trainers should be better than the prior trainer.</span></span> |<xref:Microsoft.ML.Trainers.PriorTrainer>|
