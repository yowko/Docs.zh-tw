---
title: 每個微服務的資料庫
description: 對比整合型和雲端原生應用程式中的資料儲存。
author: robvet
ms.date: 01/22/2020
ms.openlocfilehash: e472309d3dc815070fc2d2c220bf4fe00b8c29ae
ms.sourcegitcommit: 13e79efdbd589cad6b1de634f5d6b1262b12ab01
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 01/28/2020
ms.locfileid: "76794902"
---
# <a name="database-per-microservice"></a>每個微服務的資料庫

[!INCLUDE [book-preview](../../../includes/book-preview.md)]

如我們在本書中所見，雲端原生方法會改變您設計、部署和管理應用程式的方式。 它也會變更您管理和儲存資料的方式。

圖5-1 對比差異。

![雲端原生應用程式中的資料儲存](./media/distributed-data.png)

**圖 5-1**. 雲端原生應用程式中的資料管理

有經驗的開發人員很容易就能辨識圖5-1 左邊的架構。 在此整合型*應用程式*中，商務服務元件會在共用服務層級中共置，並從單一關係資料庫共用資料。

在許多方面，單一資料庫會讓資料管理變得簡單。 查詢多個資料表中的資料很簡單。 對資料更新的變更，或全部回復。 [ACID 交易](https://docs.microsoft.com/windows/desktop/cossdk/acid-properties)保證強式和立即一致性。

針對雲端原生設計，我們採用不同的方法。 在 [圖 5-1] 的右側，請注意商務功能如何都會隔離為小型、獨立的微服務。 每個微服務都會封裝特定的商務功能和自己的資料。 整合型資料庫會 decompose 到具有許多較小資料庫的分散式資料模型中，每個都與一個微服務對齊。 當冒煙清除時，我們會形成一個設計，*每個微服務*都會公開一個資料庫。

## <a name="why"></a>為什麼？

每個微服務的這個資料庫提供許多優點，特別是針對必須快速發展並支援大規模的系統。 使用此模型 。

- 網域資料封裝在服務內
- 資料架構可以進化，而不會直接影響其他服務
- 每個資料存放區都可以獨立調整
- 某個服務中的資料存放區失敗不會直接影響其他服務

分隔資料也可讓每個微服務執行最適合其工作負載、儲存體需求和讀寫模式的資料存放區類型。 選擇包括關聯式、檔、索引鍵/值，甚至是以圖表為基礎的資料存放區。

圖5-2 顯示在雲端原生系統中多語言持續性的原則。

![多語言資料持續性](./media/polyglot-data-persistence.png)

**圖 5-2**。 多語言資料持續性

請注意，上圖中的每個微服務如何支援不同類型的資料存放區。

- 產品目錄微服務會使用關係資料庫來容納其基礎資料的豐富關聯式結構。
- [購物車] 微服務會使用支援其簡單的索引鍵/值資料存放區的分散式快取。
- 訂購微服務會同時使用 NoSql 檔資料庫進行寫入作業以及高度反正規化的索引鍵/值存放區，以容納大量的讀取作業。
  
雖然關係資料庫與複雜資料的微服務保持相關，但 NoSQL 資料庫已獲得相當大的普及。 它們提供大規模和高可用性。 其無架構的本質可讓開發人員從具類型的資料類別和 Orm 的架構中移出，這會使變更成本高昂且耗時。 我們將在本章稍後討論 NoSQL 資料庫。

 雖然將資料封裝成不同的微服務可以增加靈活性、效能和擴充性，但也會帶來許多挑戰。 在下一節中，我們將討論這些挑戰以及模式和實務，以協助克服這些難題。  

## <a name="cross-service-queries"></a>跨服務查詢

雖然微服務是獨立的，而且專注于特定的功能功能，例如清查、出貨或訂購，但它們經常需要與其他微服務整合。 整合通常牽涉到一個微服務*查詢*資料的另一個。 圖5-3 顯示案例。

![跨微服務查詢](./media/cross-service-query.png)

**圖 5-3**。 跨微服務查詢

在上圖中，我們看到 [購物籃] 微服務，可將專案新增至使用者的購物籃。 雖然此微服務的資料存放區包含購物籃和明細專案資料，但它不會維護產品或定價資料。 而是由 [目錄] 和 [定價] 微服務所擁有的這些資料項目。 這會造成問題。 當購物籃在其資料庫中沒有產品或定價資料時，如何微服務將產品新增至使用者的購物籃？

第4章所討論的一個選項是從購物籃到目錄和定價微服務的[直接 HTTP 呼叫](service-to-service-communication.md#queries)。 不過，在第4章中，我們說過同步的*HTTP 呼叫數*個微服務，減少了其自我的自我整合，並降低了架構的優點。

我們也可以針對每個服務，使用個別的輸入和輸出佇列來執行要求-回復模式。 不過，此模式很複雜，而且需要進行管線以關聯要求和回應訊息。
雖然它會分離後端微服務呼叫，但呼叫服務仍必須同步等候呼叫完成。 網路擁塞、暫時性錯誤或多載微服務，可能會導致長時間執行甚至失敗的作業。

相反地，移除跨服務相依性的廣泛接受模式就是[具體化視圖模式](https://docs.microsoft.com/azure/architecture/patterns/materialized-view)，如圖5-4 所示。

![具體化視圖模式](./media/materialized-view-pattern.png)

**圖 5-4**。 具體化視圖模式

使用此模式時，您會將本機資料表（稱為「*讀取模型*」）放在「購物籃」服務中。 此資料表包含產品和定價微服務所需之資料的反正規化複本。 將資料直接複製到 [購物籃] 微服務，就不需要昂貴的跨服務呼叫。 在服務的本機資料中，您可以改善服務的回應時間和可靠性。 此外，擁有自己的資料複本，可以提高購物籃服務的彈性。 如果目錄服務應該變成無法使用，它不會直接影響購物籃服務。 購物籃可以繼續使用自己的存放區中的資料來操作。 

這種方法的 catch 在於您的系統中現在有重複的資料。 不過，在雲端原生系統中，*策略性*地複製資料是已建立的實務，並不會被視為反模式或不正確的作法。 請記住，*只有一個服務*可以擁有一個資料集，並具有其授權。 當記錄的系統更新時，您必須同步處理讀取模型。 同步處理通常是透過使用[發佈/訂閱模式](service-to-service-communication.md#events)的非同步訊息來執行，如圖5.4 所示。

## <a name="distributed-transactions"></a>分散式交易

查詢跨微服務的資料很艱難，在數個微服務之間執行交易更為複雜。 在不同微服務的獨立資料來源之間維護資料一致性的固有挑戰，無法 understated。 在雲端原生應用程式中缺少分散式交易，表示您必須以程式設計方式管理分散式交易。 您可以從*立即一致性*的世界轉移到*最終一致性*。

圖5-5 顯示問題。

![Saga 模式中的交易](./media/saga-transaction-operation.png)

**圖 5-5**。 跨微服務執行交易

在上圖中，五個獨立的微服務會參與建立訂單的分散式交易。 每個微服務都會維護它自己的資料存放區，並為其存放區執行本機交易。 若要建立訂單，*每個*個別微服務的本機交易必須成功，或*全部都*必須中止並復原作業。 雖然內建交易支援可在每個微服務中使用，但不支援橫跨所有五個服務的分散式交易，以保持資料的一致性。

相反地，您必須以程式設計*方式*來結構化此分散式交易。

加入分散式交易支援的常見模式是 Saga 模式。 它是藉由以程式設計方式將本機交易群組在一起，並依序叫用每一個。 如果任何本機交易失敗，Saga 會中止作業並叫用一組[補償交易](https://docs.microsoft.com/azure/architecture/patterns/compensating-transaction)。 補償交易會復原先前的本機交易所做的變更，並還原資料的一致性。 圖5-6 顯示具有 Saga 模式的失敗交易。

![在 saga 模式中復原](./media/saga-rollback-operation.png)

**圖 5-6**. 復原交易

在上圖中，清查微服務中的*更新清查*操作失敗。 Saga 會叫用一組補償交易（以紅色表示）來調整清查計數、取消付款和訂單，然後將每個微服務的資料傳回一致的狀態。

Saga 模式通常會單純為一系列相關的事件，或協調成一組相關的命令。 在第4章中，我們討論了「服務匯總工具」模式，這是協調的 saga 實作為基礎。 我們也討論了事件，以及 Azure 服務匯流排和 Azure Event Grid 主題，這是單純 saga 的基礎。

## <a name="high-volume-data"></a>大量資料

大型雲端原生應用程式通常支援高容量的資料需求。 在這些案例中，傳統的資料儲存技術可能會造成瓶頸。 對於大規模部署的複雜系統，命令與查詢責任隔離（CQRS）和事件來源都可以改善應用程式效能。  

### <a name="cqrs"></a>CQRS

[CQRS](https://docs.microsoft.com/azure/architecture/patterns/cqrs)是一種架構模式，可協助最大化效能、擴充性和安全性。 此模式會分隔從寫入資料的作業讀取資料的作業。 

在一般情況下 *，讀取和寫入作業都會使用*相同的實體模型和資料存放庫物件。

不過，大量資料的案例可受益于個別的模型和資料表，以進行讀取和寫入。 為了改善效能，讀取作業可以針對資料的高度反正規化標記法進行查詢，以避免昂貴的重複性資料表聯結和表鎖。 *寫入*作業（稱為*命令*）會針對會確保一致性的資料，以完全正規化的標記法來進行更新。 接著，您必須執行機制，讓這兩種標記法保持同步。一般來說，每當修改寫入資料表時，就會發行一個事件，將修改複寫到讀取資料表。

圖5-7 顯示 CQRS 模式的執行。

![命令與查詢責任隔離](./media/cqrs-implementation.png)

**圖 5-7**。 CQRS 執行

在上圖中，會執行個別的命令和查詢模型。 每個資料寫入作業都會儲存到寫入存放區，然後傳播到讀取存放區。 請密切注意資料傳播程式如何在[最終一致性](http://www.cloudcomputingpatterns.org/eventual_consistency/)原則上運作。 讀取模型最後會與寫入模型進行同步處理，但進程中可能會有一些延遲。 我們將在下一節中討論最終一致性。

這種區隔可讓讀取和寫入獨立進行調整。 讀取作業會使用針對查詢優化的架構，而寫入會使用針對更新優化的架構。 讀取查詢會針對不正規化的資料進行，而複雜的商務邏輯則可以套用至寫入模型。 同樣地，您可能會對寫入作業施加更緊密的安全性，而不是公開讀取。

執行 CQRS 可以改善雲端原生服務的應用程式效能。 不過，它會產生更複雜的設計。 請仔細且策略性地將此原則套用至雲端原生應用程式的那些區段，以從中獲益。 如需 CQRS 的詳細資訊，請參閱 Microsoft book [.Net 微服務：容器化 .Net 應用程式的架構](https://docs.microsoft.com/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/apply-simplified-microservice-cqrs-ddd-patterns)。

### <a name="event-sourcing"></a>事件來源

優化大量資料案例的另一種方法牽涉到[事件來源](https://docs.microsoft.com/azure/architecture/patterns/event-sourcing)。

系統通常會儲存資料實體的目前狀態。 例如，如果使用者變更電話號碼，就會以新的號碼更新客戶記錄。 我們一律知道資料實體的目前狀態，但每個更新都會覆寫先前的狀態。 

在大部分情況下，此模型會正常運作。 不過，在大量系統中，交易式鎖定和頻繁更新作業的額外負荷可能會影響資料庫效能、回應能力和限制擴充性。

事件來源會採用不同的方法來捕獲資料。 會影響資料的每項作業都會保存到事件存放區。 我們不會更新資料記錄的狀態，而是將每個變更附加至過去事件的連續清單，類似于會計的總帳。 事件存放區會成為資料的記錄系統。 它是用來在微服務的限定內容中傳播各種具體化的視圖。 圖5.8 顯示模式。

![事件來源](./media/event-sourcing.png)

**圖 5-8**。 事件來源

在上圖中，請注意如何將使用者購物車的每個專案（藍色）附加至基礎事件存放區。 在連續的具體化視圖中，系統會重新執行與每個購物車相關聯的所有事件，以投射目前的狀態。 這個視圖或讀取模型接著會公開回到 UI。 事件也可以與外部系統和應用程式整合，或進行查詢以判斷實體的目前狀態。 透過這種方法，您可以維護歷程記錄。 您不僅可以得知實體的目前狀態，也知道如何達到此狀態。

就說，事件來源會簡化寫入模型。 沒有更新或刪除。 將每個資料項目附加為不可變事件會將與關係資料庫相關聯的爭用、鎖定和並行衝突降到最低。 使用具體化視圖模式建立讀取模型，可讓您將此視圖與寫入模型分離，並選擇最佳的資料存放區，以將應用程式 UI 的需求優化。

在此模式中，請考慮直接支援事件來源的資料存放區。 Azure Cosmos DB、MongoDB、Cassandra、CouchDB 和 RavenDB 都是不錯的候選項目。

就像所有模式和技術一樣，在必要時進行策略性的實行。 雖然事件來源可以提供更高的效能和擴充性，但代價是複雜性和學習曲線。

>[!div class="step-by-step"]
>[上一頁](service-mesh-communication-infrastructure.md)
>[下一頁](relational-vs-nosql-data.md)
